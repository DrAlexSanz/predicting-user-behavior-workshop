{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Synerise/predicting-user-behavior-workshop.git\n",
    "%cd predicting-user-behavior-workshop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, http://artifactory.service/api/pypi/synePy/simple\n",
      "Requirement already satisfied: lightning in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (2.2.0.post0)\n",
      "Requirement already satisfied: PyYAML<8.0,>=5.4 in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from lightning) (6.0.1)\n",
      "Requirement already satisfied: fsspec<2025.0,>=2022.5.0 in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning) (2024.2.0)\n",
      "Requirement already satisfied: lightning-utilities<2.0,>=0.8.0 in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from lightning) (0.10.1)\n",
      "Requirement already satisfied: numpy<3.0,>=1.17.2 in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from lightning) (1.26.4)\n",
      "Requirement already satisfied: packaging<25.0,>=20.0 in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from lightning) (23.2)\n",
      "Requirement already satisfied: torch<4.0,>=1.13.0 in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from lightning) (2.2.1)\n",
      "Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from lightning) (1.3.1)\n",
      "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from lightning) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from lightning) (4.9.0)\n",
      "Requirement already satisfied: pytorch-lightning in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from lightning) (2.2.0.post0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning) (3.9.3)\n",
      "Requirement already satisfied: setuptools in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from lightning-utilities<2.0,>=0.8.0->lightning) (68.2.2)\n",
      "Requirement already satisfied: filelock in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from torch<4.0,>=1.13.0->lightning) (3.13.1)\n",
      "Requirement already satisfied: sympy in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from torch<4.0,>=1.13.0->lightning) (1.12)\n",
      "Requirement already satisfied: networkx in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from torch<4.0,>=1.13.0->lightning) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from torch<4.0,>=1.13.0->lightning) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from torch<4.0,>=1.13.0->lightning) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from torch<4.0,>=1.13.0->lightning) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from torch<4.0,>=1.13.0->lightning) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from torch<4.0,>=1.13.0->lightning) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from torch<4.0,>=1.13.0->lightning) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from torch<4.0,>=1.13.0->lightning) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from torch<4.0,>=1.13.0->lightning) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from torch<4.0,>=1.13.0->lightning) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from torch<4.0,>=1.13.0->lightning) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from torch<4.0,>=1.13.0->lightning) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from torch<4.0,>=1.13.0->lightning) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from torch<4.0,>=1.13.0->lightning) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<4.0,>=1.13.0->lightning) (12.3.101)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.9.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from jinja2->torch<4.0,>=1.13.0->lightning) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from sympy->torch<4.0,>=1.13.0->lightning) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.0 in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (3.6)\n",
      "Looking in indexes: https://pypi.org/simple, http://artifactory.service/api/pypi/synePy/simple\n",
      "Requirement already satisfied: torchmetrics in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (1.3.1)\n",
      "Requirement already satisfied: numpy>1.20.0 in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from torchmetrics) (1.26.4)\n",
      "Requirement already satisfied: packaging>17.1 in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from torchmetrics) (23.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from torchmetrics) (2.2.1)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from torchmetrics) (0.10.1)\n",
      "Requirement already satisfied: setuptools in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (68.2.2)\n",
      "Requirement already satisfied: typing-extensions in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.9.0)\n",
      "Requirement already satisfied: filelock in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from torch>=1.10.0->torchmetrics) (3.13.1)\n",
      "Requirement already satisfied: sympy in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
      "Requirement already satisfied: networkx in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from torch>=1.10.0->torchmetrics) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from torch>=1.10.0->torchmetrics) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from torch>=1.10.0->torchmetrics) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from torch>=1.10.0->torchmetrics) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from torch>=1.10.0->torchmetrics) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from torch>=1.10.0->torchmetrics) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from torch>=1.10.0->torchmetrics) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from torch>=1.10.0->torchmetrics) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from torch>=1.10.0->torchmetrics) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from torch>=1.10.0->torchmetrics) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from torch>=1.10.0->torchmetrics) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->torchmetrics) (12.3.101)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install lightning\n",
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn, optim, Tensor\n",
    "from torchmetrics import AveragePrecision, AUROC\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from emde import calculate_absolute_emde_codes\n",
    "from cleora_saas_api import CLI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants\n",
    "First we define all constants that will be used in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data/\"\n",
    "TRAIN_PD_PATH = os.path.join(DATA_DIR, \"2019-Oct_small.csv\")\n",
    "TARGETS_PATH = os.path.join(DATA_DIR,\"train_target.npy\")\n",
    "VALIDATION_TARGETS_PATH = os.path.join(DATA_DIR,\"test_target.npy\")\n",
    "USER_IDS = os.path.join(DATA_DIR, \"user_ids.npy\")\n",
    "CLEORA_INPUT_FILE = os.path.join(DATA_DIR,\"cleora_input.tsv\")\n",
    "EMBEDDINGS_NPZ = \"embeddings.npz\"\n",
    "SKETCH_DEPTH = 20\n",
    "SKETCH_WIDTH = 64\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.0001\n",
    "MAX_EPOCH = 1\n",
    "ACCELERATOR = \"gpu\"\n",
    "DEVICES = 1\n",
    "NUM_WORKERS = 8\n",
    "EXPERIMENT_NAME = \"experiment_with_brands\"\n",
    "CLEORA_API_TOKEN = \"AMf-vByg7WAB7GhZsOpQU_PisBJINqw-IRncFm182Ly3R7JyUxRY0JIjM9hJXoNJ9Q9ceHwWUn0Ghc60J2jrxJVyUyZyf5mQLUElpb9DJbd5q-PXHNjE_QHRXAPEKNX2relRJycP6FOw2fxf8fngHEw6CvLS44nbuxIhTDd_b1w8JNkhaPIr-8GOJAL8OlV06cEmf6iJZnLqDSkIV5msh6WaUQRV0canHZb1o30SmRBxawHYs2-n7xFEDOO-H_ULoCLVDZHBlnA1ewnEqYMQkpNWc32atG-8HmitYWIG-P-OClL8YOh53Oy95kFIN6A4u2CK46KUN9qd1edD3gFvl_vjg9auC3ZwzQ\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Calculating embeddings with Cleora\n",
    "The following function prepares input for cleora. First we load DataFrame with training data. Note that cleora works with timestamps as well. However, in our case we drop timestamps for simplicity. Finally we save the result as a tsv file, which is required input format for cleora. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_time</th>\n",
       "      <th>brand</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-10-01 00:02:14 UTC</td>\n",
       "      <td>samsung</td>\n",
       "      <td>543272936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-10-01 00:04:37 UTC</td>\n",
       "      <td>apple</td>\n",
       "      <td>551377651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-10-01 00:05:14 UTC</td>\n",
       "      <td>xiaomi</td>\n",
       "      <td>550121407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-10-01 00:06:02 UTC</td>\n",
       "      <td>xiaomi</td>\n",
       "      <td>514591159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-10-01 00:07:07 UTC</td>\n",
       "      <td>santeri</td>\n",
       "      <td>555332717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2019-10-01 00:09:26 UTC</td>\n",
       "      <td>apple</td>\n",
       "      <td>524601178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2019-10-01 00:09:33 UTC</td>\n",
       "      <td>apple</td>\n",
       "      <td>524325294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2019-10-01 00:09:54 UTC</td>\n",
       "      <td>apple</td>\n",
       "      <td>551377651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2019-10-01 00:10:08 UTC</td>\n",
       "      <td>apple</td>\n",
       "      <td>524325294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2019-10-01 00:10:56 UTC</td>\n",
       "      <td>oasis</td>\n",
       "      <td>548691404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                event_time    brand    user_id\n",
       "0  2019-10-01 00:02:14 UTC  samsung  543272936\n",
       "1  2019-10-01 00:04:37 UTC    apple  551377651\n",
       "2  2019-10-01 00:05:14 UTC   xiaomi  550121407\n",
       "3  2019-10-01 00:06:02 UTC   xiaomi  514591159\n",
       "4  2019-10-01 00:07:07 UTC  santeri  555332717\n",
       "5  2019-10-01 00:09:26 UTC    apple  524601178\n",
       "6  2019-10-01 00:09:33 UTC    apple  524325294\n",
       "7  2019-10-01 00:09:54 UTC    apple  551377651\n",
       "8  2019-10-01 00:10:08 UTC    apple  524325294\n",
       "9  2019-10-01 00:10:56 UTC    oasis  548691404"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(TRAIN_PD_PATH)\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_cleora_input_file():\n",
    "    train_df = pd.read_csv(TRAIN_PD_PATH)\n",
    "    train_df.drop(\"event_time\", axis=1, inplace=True)\n",
    "    train_df = train_df[[\"user_id\", \"brand\"]]\n",
    "    with open(CLEORA_INPUT_FILE, \"w\") as tsv_file:\n",
    "        train_df.to_csv(tsv_file, sep=\"\\t\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_cleora_input_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logged in successfully\n"
     ]
    }
   ],
   "source": [
    "cleora = CLI()\n",
    "cleora.login(CLEORA_API_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Start --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Config to trigger run prepared --\n",
      "-- Run started --\n",
      "-- Logs: --\n",
      "Started at 2024-03-21 09:10:12\n",
      "Downloading input file.\n",
      "Number of rows in original data: 1592254\n",
      "Number of rows after preprocessing: 1592254\n",
      "Number of embedded nodes: 2012\n",
      "Initializing Cleora.\n",
      "Iteration 1/3 done\n",
      "Iteration 2/3 done\n",
      "Iteration 3/3 done\n",
      "Saving results.\n",
      "-- Result download started --\n",
      "-- Result download finished --\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cleora.run(256, 3, input_path=os.path.join(DATA_DIR, \"cleora_input.tsv\"), run_name=\"colab_clustering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "Instead of using 3 iterations with dimension 256 set the number of iterations to 4 and dimension to 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Start --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Config to trigger run prepared --\n",
      "-- Run started --\n",
      "-- Logs: --\n",
      "Started at 2024-03-21 09:10:33\n",
      "Downloading input file.\n",
      "Input file downloaded with 1592254 rows.\n",
      "Number of rows after preprocessing: 1592254\n",
      "Number of embedded nodes: 2012\n",
      "Initializing Cleora.\n",
      "Iteration 1/4 done\n",
      "Iteration 3/4 done\n",
      "Iteration 4/4 done\n",
      "Saving results.\n",
      "-- Result download started --\n",
      "-- Result download finished --\n"
     ]
    }
   ],
   "source": [
    "# @title Solution\n",
    "\n",
    "cleora.run(128, 4, input_path=os.path.join(DATA_DIR, \"cleora_input.tsv\"), run_name=\"colab_clustering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is used to load embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(embeddings_path: str):\n",
    "    embeddings = np.load(embeddings_path)\n",
    "    return embeddings[\"entity_id\"], embeddings[\"vectors\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We explain now the output of cleora.ai app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings shape: (2012, 128), embeddings dtype: float32\n",
      "brands_ids shape: (2012,)\n"
     ]
    }
   ],
   "source": [
    "brands_ids, embeddings = load_embeddings(embeddings_path=EMBEDDINGS_NPZ)\n",
    "print(f\"embeddings shape: {embeddings.shape}, embeddings dtype: {embeddings.dtype}\")\n",
    "print(f\"brands_ids shape: {brands_ids.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['roca', 'delsey', 'ant', 'huanger', 'baboo', 'saferich', 'ibox',\n",
       "       'celine', 'alvitek', 'roland'], dtype='<U28')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brands_ids[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us find brand which corresponds to some index and then print its embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ant'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 2\n",
    "brands_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.0045849 ,  1.057831  ,  0.6117971 , -0.9714132 , -0.2641284 ,\n",
       "        1.3263575 , -1.3149711 , -1.5308113 ,  1.6993662 , -1.0843438 ,\n",
       "        0.6221408 , -0.9278464 ,  1.0581088 ,  1.4986578 , -0.16034423,\n",
       "        1.3795992 , -0.36579174,  0.65135425,  0.15182483, -1.0167545 ,\n",
       "       -0.2879624 , -1.2571608 , -0.627918  , -0.39504528, -1.6736643 ,\n",
       "        0.00210125, -0.93855894,  0.12950474, -1.1306771 ,  1.0367324 ,\n",
       "       -0.06042258, -1.2315695 ,  0.1774957 ,  0.41165933,  0.29261816,\n",
       "        0.10841989,  0.02514603, -1.3834397 , -1.6436853 ,  1.4555696 ,\n",
       "        1.0099329 ,  1.4808849 ,  0.03435909,  0.11983018,  0.2968295 ,\n",
       "       -1.0899298 , -0.89928   ,  0.50339437,  0.42901525,  1.0438459 ,\n",
       "        1.1279889 , -1.5118362 , -1.1328359 ,  0.32668203,  0.71964407,\n",
       "        1.4428797 ,  1.5700753 , -0.8538135 , -0.43687302,  0.10625994,\n",
       "        0.5488494 ,  0.6277199 , -0.10483881, -1.2503387 , -0.09838147,\n",
       "       -0.19720419, -0.4720139 , -0.56241417,  1.021901  , -1.6649629 ,\n",
       "        0.06446873, -0.09625839, -1.3506763 ,  1.5425384 , -1.3817792 ,\n",
       "        1.2688193 ,  1.6495134 , -1.4690309 , -1.412551  , -0.90460193,\n",
       "       -1.2842209 , -0.42915583,  0.5756823 ,  0.6703108 ,  0.23713587,\n",
       "        1.6162645 ,  0.8477841 , -1.6445135 , -1.0303534 ,  0.4560837 ,\n",
       "        0.78031063,  1.4560909 ,  0.42429435, -1.180994  ,  0.5022783 ,\n",
       "       -0.42140934, -0.5614093 , -0.1360917 , -0.40012565, -0.26460543,\n",
       "        0.40976915,  0.87073594,  0.43529806, -1.0188504 , -0.43657455,\n",
       "        1.7160562 ,  0.3229012 ,  1.4607427 , -0.04129849, -0.6246307 ,\n",
       "        1.5733845 , -1.6055007 ,  0.8494124 , -0.8336452 , -0.43029636,\n",
       "        1.0268537 ,  0.73972344,  0.6029362 , -1.0572913 , -1.6672047 ,\n",
       "       -0.43382296, -1.5378673 ,  0.94613713,  1.0155296 , -0.15783224,\n",
       "       -1.3144463 ,  1.4315008 ,  0.18387839], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Dataset class\n",
    "\n",
    "We explain here some details related to our implementation of Dataset class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we investigate the contents of training DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_time</th>\n",
       "      <th>brand</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-10-01 00:02:14 UTC</td>\n",
       "      <td>samsung</td>\n",
       "      <td>543272936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-10-01 00:04:37 UTC</td>\n",
       "      <td>apple</td>\n",
       "      <td>551377651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-10-01 00:05:14 UTC</td>\n",
       "      <td>xiaomi</td>\n",
       "      <td>550121407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-10-01 00:06:02 UTC</td>\n",
       "      <td>xiaomi</td>\n",
       "      <td>514591159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-10-01 00:07:07 UTC</td>\n",
       "      <td>santeri</td>\n",
       "      <td>555332717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2019-10-01 00:09:26 UTC</td>\n",
       "      <td>apple</td>\n",
       "      <td>524601178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2019-10-01 00:09:33 UTC</td>\n",
       "      <td>apple</td>\n",
       "      <td>524325294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2019-10-01 00:09:54 UTC</td>\n",
       "      <td>apple</td>\n",
       "      <td>551377651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2019-10-01 00:10:08 UTC</td>\n",
       "      <td>apple</td>\n",
       "      <td>524325294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2019-10-01 00:10:56 UTC</td>\n",
       "      <td>oasis</td>\n",
       "      <td>548691404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                event_time    brand    user_id\n",
       "0  2019-10-01 00:02:14 UTC  samsung  543272936\n",
       "1  2019-10-01 00:04:37 UTC    apple  551377651\n",
       "2  2019-10-01 00:05:14 UTC   xiaomi  550121407\n",
       "3  2019-10-01 00:06:02 UTC   xiaomi  514591159\n",
       "4  2019-10-01 00:07:07 UTC  santeri  555332717\n",
       "5  2019-10-01 00:09:26 UTC    apple  524601178\n",
       "6  2019-10-01 00:09:33 UTC    apple  524325294\n",
       "7  2019-10-01 00:09:54 UTC    apple  551377651\n",
       "8  2019-10-01 00:10:08 UTC    apple  524325294\n",
       "9  2019-10-01 00:10:56 UTC    oasis  548691404"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(TRAIN_PD_PATH)\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We group train Dataframe by user and aggregate obtained groups by applying list construtor. This constructs Series that contains list of interactions of every user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id\n",
       "264649825        [kiturami, kiturami]\n",
       "284344819                     [apple]\n",
       "293957954                    [xiaomi]\n",
       "303160429                    [garmin]\n",
       "304325717    [huawei, huawei, huawei]\n",
       "318611205              [huawei, zeta]\n",
       "336595257          [samsung, samsung]\n",
       "340041246        [lg, lg, lg, lg, lg]\n",
       "348815209                   [samsung]\n",
       "362327778                     [apple]\n",
       "Name: brand, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brands = train_df.groupby(\"user_id\", group_keys=True)[\"brand\"].apply(list)\n",
    "brands.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to implement our custom dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UsersBrandsDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        absolute_codes: np.ndarray,\n",
    "        brands_ids: np.ndarray,\n",
    "        train_df_path: str,\n",
    "        targets_path: str,\n",
    "        user_ids_path: str,\n",
    "        sketch_width: int,\n",
    "        sketch_depth: int,\n",
    "        sketch_decay: float = 0.94,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            absolute_codes (np.ndarray): Array of shape (num_brands, sketch_depth) containing the absolute codes for each item\n",
    "            brands_ids (np.ndarray): Array of shape (num_brands) mapping each idx to corresponding brand \n",
    "            inputs_df_path (str): path to train dataframe\n",
    "            targets_path (str): path to targets array\n",
    "            sketch_width (int): width of the sketch\n",
    "            sketch_depth (int): depth of the sketch\n",
    "            sketch_decay (float): Decay factor for the sketch\n",
    "        \"\"\"\n",
    "        self.absolute_codes = absolute_codes\n",
    "        self.sketch_depth = sketch_depth\n",
    "        self.sketch_width = sketch_width\n",
    "        self.sketch_decay = sketch_decay\n",
    "        \n",
    "        self.brand_to_ids = {brands_ids[idx]: idx for idx in range(len(brands_ids))}\n",
    "\n",
    "        train_df = pd.read_csv(train_df_path)\n",
    "        \n",
    "        self.brands = train_df.groupby(\"user_id\")[\"brand\"].apply(list)\n",
    "        self.users_ids = np.load(user_ids_path)\n",
    "        self.target_brands = np.load(targets_path)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users_ids)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        try:\n",
    "            brands = self.brands.iloc[idx]\n",
    "        except IndexError:\n",
    "            print(f\"WRONG IDS IS {idx}\")        \n",
    "        brands = [self.brand_to_ids[brand] for brand in brands]        \n",
    "        brands_codes = torch.from_numpy(self.absolute_codes[brands])\n",
    "        user_sketch = torch.zeros(self.sketch_depth * self.sketch_width, dtype=torch.float32)\n",
    "        for brand_codes in brands_codes:\n",
    "            user_sketch *= self.sketch_decay\n",
    "            user_sketch[brand_codes] += 1\n",
    "                    \n",
    "        target = self.target_brands[idx]\n",
    "        return user_sketch, target\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using PyTorch Lightning, we need to wrap our dataset in LightningDataModule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserBrandDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        brands_ids: np.array,\n",
    "        embeddings: np.array,\n",
    "        train_df_path: str,\n",
    "        targets_path: str,\n",
    "        validation_targets_path: str,\n",
    "        user_ids_path: str,\n",
    "        sketch_width: int,\n",
    "        sketch_depth: int,\n",
    "        batch_size: int,\n",
    "        num_workers: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.brands_ids = brands_ids\n",
    "        self.embeddings = embeddings\n",
    "        self.train_df_path = train_df_path\n",
    "        self.targets_path = targets_path\n",
    "        self.validation_targets_path = validation_targets_path\n",
    "        self.user_ids_path = user_ids_path\n",
    "        self.sketch_depth = sketch_depth\n",
    "        self.sketch_width = sketch_width\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def setup(self, stage) -> None:\n",
    "        if stage == \"fit\":\n",
    "            absolute_emde_codes = calculate_absolute_emde_codes(self.sketch_depth, self.sketch_width, self.embeddings)\n",
    "            self.train_data = UsersBrandsDataset(\n",
    "                absolute_codes=absolute_emde_codes,\n",
    "                brands_ids=self.brands_ids,\n",
    "                train_df_path=self.train_df_path,\n",
    "                targets_path=self.targets_path,\n",
    "                user_ids_path=self.user_ids_path,\n",
    "                sketch_depth=self.sketch_depth,\n",
    "                sketch_width=self.sketch_width,\n",
    "            )\n",
    "            self.validation_data = UsersBrandsDataset(\n",
    "                absolute_codes=absolute_emde_codes,\n",
    "                brands_ids=self.brands_ids,\n",
    "                train_df_path=self.train_df_path,\n",
    "                targets_path=self.validation_targets_path,\n",
    "                user_ids_path=self.user_ids_path,\n",
    "                sketch_depth=self.sketch_depth,\n",
    "                sketch_width=self.sketch_width,\n",
    "            )\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.train_data, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.validation_data,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining simple feedforward Neural Network\n",
    "\n",
    "Below we implement simple feedforward neural network with binary cross entropy loss and multilabel auroc as validation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_size: int,\n",
    "        output_dim: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.Linear(hidden_size, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        return self.linear_relu_stack(x)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_size: int,\n",
    "        output_dim: int,\n",
    "        learning_rate: float,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.net = Net(hidden_size=hidden_size, input_dim=input_dim, output_dim=output_dim)\n",
    "        self.val_auroc = AUROC(task=\"multilabel\", num_labels=output_dim)\n",
    "\n",
    "    def forward(self, x) -> Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "    def configure_optimizers(self) -> optim.Optimizer:\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx) -> Tensor:\n",
    "        x, y = train_batch\n",
    "        preds = self.forward(x)\n",
    "        loss = F.binary_cross_entropy_with_logits(preds, y)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx) -> None:\n",
    "        x, y = val_batch\n",
    "        preds = self.forward(x)\n",
    "        loss = F.binary_cross_entropy_with_logits(preds, y)\n",
    "        self.val_auroc(preds, y.long())\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_epoch=True, logger=True)\n",
    "\n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        self.log(\"val_auroc\", self.val_auroc, prog_bar=True, on_epoch=True, logger=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and results\n",
    "\n",
    "Now we combine all these elements together into a piece of code which trains our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need calculate number of target brands, since this is the ouput size of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_target_brands = np.load(TARGETS_PATH).shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we load embeddings and brands_ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "brands_ids, embeddings = load_embeddings(embeddings_path=EMBEDDINGS_NPZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are able to construct data module and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = UserBrandDataModule(\n",
    "    brands_ids=brands_ids,\n",
    "    embeddings=embeddings,\n",
    "    train_df_path=TRAIN_PD_PATH,\n",
    "    targets_path=TARGETS_PATH,\n",
    "    validation_targets_path=VALIDATION_TARGETS_PATH,\n",
    "    user_ids_path=USER_IDS,\n",
    "    sketch_width=SKETCH_WIDTH,\n",
    "    sketch_depth=SKETCH_DEPTH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "\n",
    "model = Model(\n",
    "    input_dim=SKETCH_DEPTH * SKETCH_WIDTH, hidden_size=2048, output_dim=num_target_brands, learning_rate=LEARNING_RATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to print some useful messages concerning training progress, current loss and validation scores. In order to to do this we add some basic logger. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = TensorBoardLogger(save_dir=\"logs\", name=f\"{EXPERIMENT_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we employ PyTorch Lightning Trainer class to wrap all configurations concerning training and validation together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    accelerator=ACCELERATOR,\n",
    "    devices=DEVICES,\n",
    "    max_epochs=MAX_EPOCH,\n",
    "    logger=logger,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now by call to fit method on trainer with model and data as arguments in order to train and validate our pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages/sklearn/random_projection.py:408: DataDimensionalityWarning: The number of components is higher than the number of features: n_features < n_components (128 < 320).The dimensionality of the problem will not be reduced.\n",
      "  warnings.warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name      | Type            | Params\n",
      "----------------------------------------------\n",
      "0 | net       | Net             | 11.1 M\n",
      "1 | val_auroc | MultilabelAUROC | 0     \n",
      "----------------------------------------------\n",
      "11.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.1 M    Total params\n",
      "44.278    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "851b3ead93e0489796912a56ad66f1af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83da131a8f4f40eda17d7025259bfbd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f066ef3286547e1be50c4490a101875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "Consider the following code, which adds additional linear layer on top of previously defined model. \n",
    "\n",
    "    class DeepNet(torch.nn.Module):\n",
    "        def __init__(\n",
    "            self,\n",
    "            input_dim: int,\n",
    "            hidden_size: int,\n",
    "            output_dim: int,\n",
    "        ) -> None:\n",
    "            super().__init__()\n",
    "            self.linear_relu_stack = nn.Sequential(\n",
    "                nn.Linear(input_dim, hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(hidden_size),\n",
    "                nn.Linear(hidden_size, hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(hidden_size),\n",
    "                nn.Linear(hidden_size, hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(hidden_size),\n",
    "                nn.Linear(hidden_size, hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(hidden_size),\n",
    "            )\n",
    "\n",
    "        def forward(self, x) -> torch.Tensor:\n",
    "            return self.linear_relu_stack(x)\n",
    "\n",
    "Replace Net with DeepNet in appropriate cell above and try to run the the training.\n",
    "\n",
    "**Q1:** Do you know what went wrong?\n",
    "**Q2:** Can you fix it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Solution\n",
    "\n",
    "# output of the sequential network has incorrect dimension!!!\n",
    "\n",
    "\n",
    "class DeepNet(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_size: int,\n",
    "        output_dim: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.Linear(hidden_size, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        return self.linear_relu_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
