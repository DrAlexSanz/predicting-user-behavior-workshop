{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn, optim, Tensor\n",
    "from torchmetrics import AveragePrecision, AUROC\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from emde import calculate_absolute_emde_codes\n",
    "from cleora_saas_api import CLI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enviroment\n",
    "First we define all constants that will be used in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/data1/lsienkiewicz/workshop\"\n",
    "TRAIN_PD_PATH = \"2019-Oct_small.csv\"\n",
    "TARGETS_PATH = \"train_target.npy\"\n",
    "VALIDATION_TARGETS_PATH = \"test_target.npy\"\n",
    "USER_IDS = \"user_ids.npy\"\n",
    "CLEORA_INPUT_FILE = \"cleora_input.tsv\"\n",
    "EMBEDDINGS_NPZ = \"embeddings.npz\"\n",
    "SKETCH_DEPTH = 20\n",
    "SKETCH_WIDTH = 64\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.0001\n",
    "MAX_EPOCH = 1\n",
    "ACCELERATOR = \"gpu\"\n",
    "DEVICES = 1\n",
    "NUM_WORKERS = 8\n",
    "EXPERIMENT_NAME = \"experiment_with_brands\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Calculating embeddings with Cleora\n",
    "The following function prepares input for cleora. First we load DataFrame with training data. Note that cleora works with timestamps as well. However, in our case we drop timestamps for simplicity. Finally we save the result as a tsv file, which is required input format for cleora. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_cleora_input_file():\n",
    "    train_df = pd.read_csv(TRAIN_PD_PATH)\n",
    "    train_df.drop(\"event_time\", axis=1, inplace=True)\n",
    "    train_df = train_df[[\"user_id\", \"brand\"]]\n",
    "    with open(CLEORA_INPUT_FILE, \"w\") as tsv_file:\n",
    "        train_df.to_csv(tsv_file, sep=\"\\t\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_cleora_input_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logged in successfully\n",
      "runId = WTJ6z1wL0QqBpC44BVhd \n",
      " \t name = colab_clustering | inputName = cleora_input.tsv \n",
      "\n",
      "runId = Wm37T8mkV0bwDCbNt1ea \n",
      " \t name = cleora_input.tsv_D256_I4 | inputName = cleora_input.tsv \n",
      "\n",
      "runId = lEMzy2jkWd1SpxUqrts7 \n",
      " \t name = colab_clustering | inputName = cleora_input.tsv \n",
      "\n",
      "runId = oq5P1ohX0NvP1cRfVre1 \n",
      " \t name = colab_clustering | inputName = cleora_input.tsv \n",
      "\n",
      "runId = vja1yPJHmgCXkrKI12RL \n",
      " \t name = colab_clustering | inputName = cleora_input.tsv \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from cleora_saas_api import CLI\n",
    "\n",
    "cleora = CLI()\n",
    "cleora.login(\n",
    "    \"AMf-vByg7WAB7GhZsOpQU_PisBJINqw-IRncFm182Ly3R7JyUxRY0JIjM9hJXoNJ9Q9ceHwWUn0Ghc60J2jrxJVyUyZyf5mQLUElpb9DJbd5q-PXHNjE_QHRXAPEKNX2relRJycP6FOw2fxf8fngHEw6CvLS44nbuxIhTDd_b1w8JNkhaPIr-8GOJAL8OlV06cEmf6iJZnLqDSkIV5msh6WaUQRV0canHZb1o30SmRBxawHYs2-n7xFEDOO-H_ULoCLVDZHBlnA1ewnEqYMQkpNWc32atG-8HmitYWIG-P-OClL8YOh53Oy95kFIN6A4u2CK46KUN9qd1edD3gFvl_vjg9auC3ZwzQ\"\n",
    ")\n",
    "cleora.show_runs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Start --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Config to trigger run prepared --\n",
      "-- Run started --\n",
      "-- Logs: --\n",
      "Started at 2024-03-20 12:22:07\n",
      "Input file downloaded with 1669365 rows.\n",
      "Number of rows in original data: 1669365\n",
      "Number of rows after preprocessing: 1592254\n",
      "Number of embedded nodes: 2012\n",
      "Initializing Cleora.\n",
      "Iteration 1/4 done\n",
      "Iteration 2/4 done\n",
      "Iteration 3/4 done\n",
      "Iteration 4/4 done\n",
      "Saving results.\n",
      "-- Result download started --\n",
      "-- Result download finished --\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cleora.run(128, 4, input_path=\"cleora_input.tsv\", run_name=\"colab_clustering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we open cleora.ai, which is web app that implements cleora algorithm. We run app with default initial embeddings and embedding dimension equal to 256. Finally we upload output embeddings and save them in the path specified by EMBEDDINGS_NPZ constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is used to load embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(embeddings_path: str):\n",
    "    embeddings = np.load(embeddings_path)\n",
    "    return embeddings[\"entity_id\"], embeddings[\"vectors\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We explain now the output of cleora.ai app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings shape: (2012, 128), embeddings dtype: float32\n",
      "brands_ids shape: (2012,)\n"
     ]
    }
   ],
   "source": [
    "brands_ids, embeddings = load_embeddings(embeddings_path=EMBEDDINGS_NPZ)\n",
    "print(f\"embeddings shape: {embeddings.shape}, embeddings dtype: {embeddings.dtype}\")\n",
    "print(f\"brands_ids shape: {brands_ids.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['misty', 'cyberpower', 'maxwell', 'shakira', 'panasonic',\n",
       "       'samsonite', 'dauscher', 'collistar', 'tsubaki', 'gemei'],\n",
       "      dtype='<U28')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brands_ids[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us find brand which corresponds to some index and then print its embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'attribute'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 121\n",
    "brands_ids[121]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.0500007 , -1.2169027 ,  1.8282223 , -0.01711618, -0.02667308,\n",
       "        0.49978656,  1.6075191 , -0.6248146 ,  1.0568687 , -0.4983308 ,\n",
       "       -0.6474467 ,  0.2727086 ,  1.5936657 ,  0.5171974 , -2.072433  ,\n",
       "       -1.4648963 ,  0.2067378 , -0.60103714,  0.07396353,  0.12636733,\n",
       "       -0.2800087 ,  0.18029667, -1.2833638 ,  0.49473673, -0.20840473,\n",
       "       -1.6134431 ,  0.09388947,  1.1613934 , -0.3771942 ,  1.2553425 ,\n",
       "       -1.3909296 ,  0.4226285 ,  0.41898215, -0.1103825 ,  1.8152021 ,\n",
       "       -0.23743178, -0.48353943, -1.0445714 , -0.16908994, -1.9817761 ,\n",
       "       -1.2700906 , -1.1548795 ,  0.82528865,  1.861263  ,  0.48486412,\n",
       "        0.10013947,  0.2618105 ,  0.8240603 , -0.21977077, -0.26917735,\n",
       "       -0.77202827, -0.316491  , -2.8853774 ,  1.04969   ,  0.62365454,\n",
       "        0.36155334,  1.1546487 , -0.8376049 ,  0.5490513 ,  1.2329954 ,\n",
       "       -0.90968317,  0.67430407, -1.9055952 ,  0.09410735,  0.06974255,\n",
       "        2.1320164 , -0.30753285, -0.29606095,  1.0193535 , -0.09628901,\n",
       "        0.7465725 ,  1.4804299 , -0.42184415, -1.6030653 , -1.0139732 ,\n",
       "        0.276043  , -1.4785328 ,  0.41628242,  0.7417735 , -0.9574699 ,\n",
       "       -0.9549018 , -0.5761184 ,  0.4757833 ,  1.054586  , -0.97688615,\n",
       "        1.6647265 , -1.0296158 ,  0.11738089,  0.6679072 ,  0.08545625,\n",
       "        1.1974447 , -0.32140917,  1.1406494 ,  0.78397804, -0.5885109 ,\n",
       "        2.0714085 , -1.0514255 ,  0.36788106, -1.8063053 , -0.08151452,\n",
       "       -0.03028374, -1.2364484 , -0.6813974 ,  0.17866997,  0.03942316,\n",
       "        1.9832582 , -0.35579133,  0.24622577, -0.39910093,  0.62938684,\n",
       "       -1.050319  ,  0.8077129 ,  0.603413  , -2.0484958 , -0.11303632,\n",
       "        0.16437279,  0.7434611 ,  0.7074605 , -0.14374283, -1.2421952 ,\n",
       "        0.7807855 ,  0.95554626,  0.00295865,  0.22392613,  1.197469  ,\n",
       "       -0.45399025, -0.64653313,  1.4068483 ], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[121]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Dataset class\n",
    "\n",
    "We explain here some details related to our implementation of Dataset class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we investigate the contents of training DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_time</th>\n",
       "      <th>brand</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-10-01 00:02:14 UTC</td>\n",
       "      <td>samsung</td>\n",
       "      <td>543272936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-10-01 00:04:37 UTC</td>\n",
       "      <td>apple</td>\n",
       "      <td>551377651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-10-01 00:05:14 UTC</td>\n",
       "      <td>xiaomi</td>\n",
       "      <td>550121407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-10-01 00:06:02 UTC</td>\n",
       "      <td>xiaomi</td>\n",
       "      <td>514591159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-10-01 00:07:07 UTC</td>\n",
       "      <td>santeri</td>\n",
       "      <td>555332717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2019-10-01 00:09:26 UTC</td>\n",
       "      <td>apple</td>\n",
       "      <td>524601178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2019-10-01 00:09:33 UTC</td>\n",
       "      <td>apple</td>\n",
       "      <td>524325294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2019-10-01 00:09:54 UTC</td>\n",
       "      <td>apple</td>\n",
       "      <td>551377651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2019-10-01 00:10:08 UTC</td>\n",
       "      <td>apple</td>\n",
       "      <td>524325294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2019-10-01 00:10:56 UTC</td>\n",
       "      <td>oasis</td>\n",
       "      <td>548691404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                event_time    brand    user_id\n",
       "0  2019-10-01 00:02:14 UTC  samsung  543272936\n",
       "1  2019-10-01 00:04:37 UTC    apple  551377651\n",
       "2  2019-10-01 00:05:14 UTC   xiaomi  550121407\n",
       "3  2019-10-01 00:06:02 UTC   xiaomi  514591159\n",
       "4  2019-10-01 00:07:07 UTC  santeri  555332717\n",
       "5  2019-10-01 00:09:26 UTC    apple  524601178\n",
       "6  2019-10-01 00:09:33 UTC    apple  524325294\n",
       "7  2019-10-01 00:09:54 UTC    apple  551377651\n",
       "8  2019-10-01 00:10:08 UTC    apple  524325294\n",
       "9  2019-10-01 00:10:56 UTC    oasis  548691404"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_df = pd.read_csv(TRAIN_PD_PATH).dropna()\n",
    "inputs_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We group train Dataframe by user and aggregate obtained groups by applying list construtor. This constructs Series that contains list of interactions of every user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id\n",
       "264649825        [kiturami, kiturami]\n",
       "284344819                     [apple]\n",
       "293957954                    [xiaomi]\n",
       "303160429                    [garmin]\n",
       "304325717    [huawei, huawei, huawei]\n",
       "318611205              [huawei, zeta]\n",
       "336595257          [samsung, samsung]\n",
       "340041246        [lg, lg, lg, lg, lg]\n",
       "348815209                   [samsung]\n",
       "362327778                     [apple]\n",
       "Name: brand, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brands = inputs_df.groupby(\"user_id\", group_keys=True)[\"brand\"].apply(list)\n",
    "brands.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([264649825, 284344819, 293957954, 303160429, 304325717, 318611205,\n",
       "       336595257, 340041246, 348815209, 362327778])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_ids = np.load(USER_IDS)\n",
    "user_ids[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to implement our custom dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UsersBrandsDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        absolute_codes: np.ndarray,\n",
    "        brands_ids: np.ndarray,\n",
    "        inputs_df_path: str,\n",
    "        targets_path: str,\n",
    "        user_ids_path: str,\n",
    "        sketch_width: int,\n",
    "        sketch_depth: int,\n",
    "        sketch_decay: float = 0.94,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            absolute_codes (np.ndarray): Array of shape (num_brands, sketch_depth) containing the absolute codes for each item\n",
    "            brands_ids (np.ndarray): Array of shape (num_brands) mapping each idx to corresponding brand \n",
    "            inputs_df_path (str): path to train dataframe\n",
    "            targets_path (str): path to targets array\n",
    "            sketch_width (int): width of the sketch\n",
    "            sketch_depth (int): depth of the sketch\n",
    "            sketch_decay (float): Decay factor for the sketch\n",
    "        \"\"\"\n",
    "        self.absolute_codes = absolute_codes\n",
    "        self.sketch_depth = sketch_depth\n",
    "        self.sketch_width = sketch_width\n",
    "        self.sketch_decay = sketch_decay\n",
    "        \n",
    "        self.brand_to_ids = {brands_ids[idx]: idx for idx in range(len(brands_ids))}\n",
    "\n",
    "        inputs_df = pd.read_csv(inputs_df_path).dropna()\n",
    "        \n",
    "        self.brands = inputs_df.groupby(\"user_id\")[\"brand\"].apply(list)\n",
    "        inputs_ids_to_users = self.brands.index.values\n",
    "        \n",
    "        targets = np.load(targets_path)\n",
    "        self.target_brands = targets[\"targets\"]\n",
    "        target_ids_to_users = targets[\"user_ids\"]\n",
    "        target_users_to_ids = {target_ids_to_users[idx]: idx for idx in range(len(target_ids_to_users))}\n",
    "        \n",
    "        self.inputs_ids_to_targets_ids = [target_users_to_ids[inputs_ids_to_users[idx]] for idx in range(len(inputs_ids_to_users))]\n",
    "        \n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs_ids_to_targets_ids)\n",
    "\n",
    "    def __getitem__(self, idx: int):        \n",
    "        brands = self.brands.iloc[idx]        \n",
    "        brands = [self.brand_to_ids[brand] for brand in brands]        \n",
    "        brands_codes = torch.from_numpy(self.absolute_codes[brands])\n",
    "        user_sketch = torch.zeros(self.sketch_depth * self.sketch_width, dtype=torch.float32)\n",
    "        for brand_codes in brands_codes:\n",
    "            user_sketch *= self.sketch_decay\n",
    "            user_sketch[brand_codes] += 1\n",
    "\n",
    "        target_idx = self.inputs_ids_to_targets_ids[idx]\n",
    "        target = self.target_brands[target_idx]\n",
    "        return user_sketch, target\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using PyTorch Lightning, we need to wrap our dataset in LightningDataModule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserBrandDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        brands_ids: np.array,\n",
    "        embeddings: np.array,\n",
    "        inputs_df_path: str,\n",
    "        targets_path: str,\n",
    "        validation_targets_path: str,\n",
    "        sketch_width: int,\n",
    "        sketch_depth: int,\n",
    "        batch_size: int,\n",
    "        num_workers: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.brands_ids = brands_ids\n",
    "        self.embeddings = embeddings\n",
    "        self.inputs_df_path = inputs_df_path\n",
    "        self.targets_path = targets_path\n",
    "        self.validation_targets_path = validation_targets_path\n",
    "        self.sketch_depth = sketch_depth\n",
    "        self.sketch_width = sketch_width\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def setup(self, stage) -> None:\n",
    "        if stage == \"fit\":\n",
    "            absolute_emde_codes = calculate_absolute_emde_codes(self.sketch_depth, self.sketch_width, self.embeddings)\n",
    "            self.train_data = UsersBrandsDataset(\n",
    "                absolute_codes=absolute_emde_codes,\n",
    "                brands_ids=self.brands_ids,\n",
    "                inputs_df_path=self.inputs_df_path,\n",
    "                targets_path=self.targets_path,\n",
    "                sketch_depth=self.sketch_depth,\n",
    "                sketch_width=self.sketch_width,\n",
    "            )\n",
    "            self.validation_data = UsersBrandsDataset(\n",
    "                absolute_codes=absolute_emde_codes,\n",
    "                brands_ids=self.brands_ids,\n",
    "                inputs_df_path=self.inputs_df_path,\n",
    "                targets_path=self.validation_targets_path,\n",
    "                sketch_depth=self.sketch_depth,\n",
    "                sketch_width=self.sketch_width,\n",
    "            )\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.train_data, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.validation_data,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining simple feedforward Neural Network\n",
    "\n",
    "Below we implement simple feedforward neural network with binary cross entropy loss and multilabel auroc as validation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_size: int,\n",
    "        output_dim: int,\n",
    "        learning_rate: float,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.Linear(hidden_size, output_dim),\n",
    "        )\n",
    "        self.val_auroc = AUROC(task=\"multilabel\", num_labels=output_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x) -> Tensor:\n",
    "        return self.linear_relu_stack(x)\n",
    "\n",
    "    def configure_optimizers(self) -> optim.Optimizer:\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx) -> Tensor:\n",
    "        x, y = train_batch\n",
    "        preds = self.forward(x)\n",
    "        loss = F.binary_cross_entropy_with_logits(preds, y)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx) -> None:\n",
    "        x, y = val_batch\n",
    "        preds = self.forward(x)\n",
    "        loss = F.binary_cross_entropy_with_logits(preds, y)\n",
    "        self.val_auroc(preds, y.long())\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_epoch=True, logger=True)\n",
    " \n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        self.log(\"val_auroc\", self.val_auroc, prog_bar=True, on_epoch=True, logger=True)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and results\n",
    "\n",
    "Now we combine all these elements together into a piece of code which trains our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need calculate number of target brands, since this is the ouput size of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_target_brands = np.load(TARGETS_PATH)[\"targets\"].shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we load embeddings and brands_ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "brands_ids, embeddings = load_embeddings(embeddings_path=EMBEDDINGS_NPZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are able to construct data module and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = UserBrandDataModule(\n",
    "    brands_ids=brands_ids,\n",
    "    embeddings=embeddings,\n",
    "    inputs_df_path=TRAIN_PD_PATH,\n",
    "    targets_path=TARGETS_PATH,\n",
    "    validation_targets_path=VALIDATION_TARGETS_PATH,\n",
    "    sketch_width=SKETCH_WIDTH,\n",
    "    sketch_depth=SKETCH_DEPTH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "\n",
    "model = Model(\n",
    "    input_dim=SKETCH_DEPTH * SKETCH_WIDTH, hidden_size=2048, output_dim=num_target_brands, learning_rate=LEARNING_RATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to print some useful messages concerning training progress, current loss and validation scores. In order to to do this we add some basic logger. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = TensorBoardLogger(save_dir=\"logs\", name=f\"{EXPERIMENT_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we employ PyTorch Lightning Trainer class to wrap all configurations concerning training and validation together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    accelerator=ACCELERATOR,\n",
    "    devices=DEVICES,\n",
    "    max_epochs=MAX_EPOCH,\n",
    "    logger=logger,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now by call to fit method on trainer with model and data as arguments in order to train and validate our pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages/sklearn/random_projection.py:408: DataDimensionalityWarning: The number of components is higher than the number of features: n_features < n_components (128 < 320).The dimensionality of the problem will not be reduced.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name              | Type            | Params\n",
      "------------------------------------------------------\n",
      "0 | linear_relu_stack | Sequential      | 11.1 M\n",
      "1 | val_auroc         | MultilabelAUROC | 0     \n",
      "------------------------------------------------------\n",
      "11.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.1 M    Total params\n",
      "44.278    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "736aa90a2717456ba7238042068d1dce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/lsienkiewicz/miniconda/envs/minenv/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d2a50a7cc7645d4be59c174d9a770d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef1c0ba351384806b3fee9caa9793148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
